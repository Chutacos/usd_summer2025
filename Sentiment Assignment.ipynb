{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f79baf9",
   "metadata": {},
   "source": [
    "# ADS 509 Sentiment Assignment\n",
    "\n",
    "This notebook holds the Sentiment Assignment for Module 6 in ADS 509, Applied Text Mining. Work through this notebook, writing code and answering questions where required. \n",
    "\n",
    "In a previous assignment you put together Twitter data and lyrics data on two artists. In this assignment we apply sentiment analysis to those data sets. If, for some reason, you did not complete that previous assignment, data to use for this assignment can be found in the assignment materials section of Blackboard. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae8e2e1",
   "metadata": {},
   "source": [
    "## General Assignment Instructions\n",
    "\n",
    "These instructions are included in every assignment, to remind you of the coding standards for the class. Feel free to delete this cell after reading it. \n",
    "\n",
    "One sign of mature code is conforming to a style guide. We recommend the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html). If you use a different style guide, please include a cell with a link. \n",
    "\n",
    "Your code should be relatively easy-to-read, sensibly commented, and clean. Writing code is a messy process, so please be sure to edit your final submission. Remove any cells that are not needed or parts of cells that contain unnecessary code. Remove inessential `import` statements and make sure that all such statements are moved into the designated cell. \n",
    "\n",
    "Make use of non-code cells for written commentary. These cells should be grammatical and clearly written. In some of these cells you will have questions to answer. The questions will be marked by a \"Q:\" and will have a corresponding \"A:\" spot for you. *Make sure to answer every question marked with a `Q:` for full credit.* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2d096b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from string import punctuation\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6b555ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     /Users/joseguarneros/nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/joseguarneros/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/joseguarneros/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add any additional import statements you need here\n",
    "\n",
    "import glob\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import download\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "download('opinion_lexicon')\n",
    "download('punkt')\n",
    "download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2b92cd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "positive_emojis = {'üòä', 'üòç', 'ü•∞', 'üòÑ', 'üòÅ', 'üëç', '‚ù§Ô∏è', 'üî•', '‚ú®', 'ü•≥'}\n",
    "negative_emojis = {'üò¢', 'üò°', 'üò≠', 'üíî', 'üëé', 'üò†', 'üòû', 'üò©', 'üò§', 'üòî'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cff4b93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aaddd590",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib.cm' has no attribute 'register_cmap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [51]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/seaborn/__init__.py:9\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcategorical\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatrix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmiscplot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maxisgrid\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/seaborn/matrix.py:12\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hierarchy\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cm\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maxisgrid\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Grid\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     15\u001b[0m     despine,\n\u001b[1;32m     16\u001b[0m     axis_ticklabels_overlap,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     _draw_figure,\n\u001b[1;32m     20\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/seaborn/cm.py:1582\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1579\u001b[0m     _cmap_r \u001b[38;5;241m=\u001b[39m colors\u001b[38;5;241m.\u001b[39mListedColormap(_lut[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], _name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_r\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \n\u001b[1;32m   1580\u001b[0m     \u001b[38;5;28mlocals\u001b[39m()[_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_r\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _cmap_r\n\u001b[0;32m-> 1582\u001b[0m     \u001b[43mmpl_cm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_cmap\u001b[49m(_name, _cmap)\n\u001b[1;32m   1583\u001b[0m     mpl_cm\u001b[38;5;241m.\u001b[39mregister_cmap(_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_r\u001b[39m\u001b[38;5;124m\"\u001b[39m, _cmap_r)\n\u001b[1;32m   1585\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m colors, mpl_cm\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'matplotlib.cm' has no attribute 'register_cmap'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "923b5a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change `data_location` to the location of the folder on your machine.\n",
    "data_location = \"M1 Results 2/\"\n",
    "\n",
    "# These subfolders should still work if you correctly stored the \n",
    "# data from the Module 1 assignment\n",
    "twitter_folder = \"twitter/\"\n",
    "lyrics_folder = \"lyrics/\"\n",
    "\n",
    "positive_words_file = \"positive-words.txt\"\n",
    "negative_words_file = \"negative-words.txt\"\n",
    "tidy_text_file = \"tidytext_sentiments.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5cf9e6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_folder = os.path.join(data_location, \"twitter\")\n",
    "lyrics_folder = os.path.join(data_location, \"lyrics/cher\")\n",
    "def read_txts_to_df(folder_path):\n",
    "    data = []\n",
    "    txt_files = glob.glob(os.path.join(folder_path, \"*.txt\"))\n",
    "    for filepath in txt_files:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                filename = os.path.basename(filepath)\n",
    "                data.append({\"filename\": filename, \"text\": content})\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3bf93e",
   "metadata": {},
   "source": [
    "## Data Input\n",
    "\n",
    "Now read in each of the corpora. For the lyrics data, it may be convenient to store the entire contents of the file to make it easier to inspect the titles individually, as you'll do in the last part of the assignment. In the solution, I stored the lyrics data in a dictionary with two dimensions of keys: artist and song. The value was the file contents. A Pandas data frame would work equally well. \n",
    "\n",
    "For the Twitter data, we only need the description field for this assignment. Feel free all the descriptions read it into a data structure. In the solution, I stored the descriptions as a dictionary of lists, with the key being the artist. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37d70801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cher_comeandstaywithme.txt</td>\n",
       "      <td>\"Come And Stay With Me\"\\n\\n\\n\\nI'll send away ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cher_pirate.txt</td>\n",
       "      <td>\"Pirate\"\\n\\n\\n\\nHe'll sail on with the summer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cher_stars.txt</td>\n",
       "      <td>\"Stars\"\\n\\n\\n\\nI was never one for saying what...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cher_thesedays.txt</td>\n",
       "      <td>\"These Days\"\\n\\n\\n\\nWell I've been out walking...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cher_lovesohigh.txt</td>\n",
       "      <td>\"Love So High\"\\n\\n\\n\\nEvery morning I would wa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     filename  \\\n",
       "0  cher_comeandstaywithme.txt   \n",
       "1             cher_pirate.txt   \n",
       "2              cher_stars.txt   \n",
       "3          cher_thesedays.txt   \n",
       "4         cher_lovesohigh.txt   \n",
       "\n",
       "                                                text  \n",
       "0  \"Come And Stay With Me\"\\n\\n\\n\\nI'll send away ...  \n",
       "1  \"Pirate\"\\n\\n\\n\\nHe'll sail on with the summer ...  \n",
       "2  \"Stars\"\\n\\n\\n\\nI was never one for saying what...  \n",
       "3  \"These Days\"\\n\\n\\n\\nWell I've been out walking...  \n",
       "4  \"Love So High\"\\n\\n\\n\\nEvery morning I would wa...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the lyrics data\n",
    "lyrics_df = read_txts_to_df(lyrics_folder)\n",
    "lyrics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "debcac5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cher_followers_data.txt</td>\n",
       "      <td>screen_name\\tname\\tid\\tlocation\\tfollowers_cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>robynkonichiwa_followers_data.txt</td>\n",
       "      <td>screen_name\\tname\\tid\\tlocation\\tfollowers_cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cher_followers.txt</td>\n",
       "      <td>id\\n35152213\\n742153090850164742\\n149646300645...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>robynkonichiwa_followers.txt</td>\n",
       "      <td>id\\n1424055675030806529\\n1502717352575651840\\n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            filename  \\\n",
       "0            cher_followers_data.txt   \n",
       "1  robynkonichiwa_followers_data.txt   \n",
       "2                 cher_followers.txt   \n",
       "3       robynkonichiwa_followers.txt   \n",
       "\n",
       "                                                text  \n",
       "0  screen_name\\tname\\tid\\tlocation\\tfollowers_cou...  \n",
       "1  screen_name\\tname\\tid\\tlocation\\tfollowers_cou...  \n",
       "2  id\\n35152213\\n742153090850164742\\n149646300645...  \n",
       "3  id\\n1424055675030806529\\n1502717352575651840\\n...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the twitter data\n",
    "twitter_df = read_txts_to_df(twitter_folder)\n",
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af9e7a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the positive and negative words and the\n",
    "# tidytext sentiment. Store these so that the positive\n",
    "# words are associated with a score of +1 and negative words\n",
    "# are associated with a score of -1. You can use a dataframe or a \n",
    "# dictionary for this.\n",
    "\n",
    "positive_words = pd.DataFrame({'word': opinion_lexicon.positive(), 'score': 1})\n",
    "negative_words = pd.DataFrame({'word': opinion_lexicon.negative(), 'score': -1})\n",
    "sentiment_df = pd.concat([positive_words, negative_words], ignore_index=True)\n",
    "\n",
    "def tokenize_and_score(text, sentiment_df):\n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "\n",
    "    text = re.sub(r'\\\\n|\\\\r|\\\\t', ' ', text) \n",
    "    text = re.sub(r'[\"\\']', '', text)       \n",
    "    text = text.replace('\\n', ' ')         \n",
    "    text = text.strip().lower()              \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) \n",
    "    \n",
    "    tokens = text.split()\n",
    "    tokens = [token for token in tokens if token not in sw and token.isalpha()]\n",
    "    \n",
    "    token_df = pd.DataFrame({'word': tokens})\n",
    "    merged = token_df.merge(sentiment_df, on='word', how='left').fillna(0)\n",
    "    \n",
    "    return merged['score'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5f3b12",
   "metadata": {},
   "source": [
    "## Sentiment Analysis on Songs\n",
    "\n",
    "In this section, score the sentiment for all the songs for both artists in your data set. Score the sentiment by manually calculating the sentiment using the combined lexicons provided in this repository. \n",
    "\n",
    "After you have calculated these sentiments, answer the questions at the end of this section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "664f8d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "lyrics_df['sentiment_score'] = lyrics_df['text'].apply(lambda x: tokenize_and_score(x, sentiment_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b7cca1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cher_comeandstaywithme.txt</td>\n",
       "      <td>\"come and stay with me\" i'll send away all my ...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cher_pirate.txt</td>\n",
       "      <td>\"pirate\" he'll sail on with the summer wind th...</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cher_stars.txt</td>\n",
       "      <td>\"stars\" i was never one for saying what i real...</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cher_thesedays.txt</td>\n",
       "      <td>\"these days\" well i've been out walking and i ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cher_lovesohigh.txt</td>\n",
       "      <td>\"love so high\" every morning i would wake up a...</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     filename  \\\n",
       "0  cher_comeandstaywithme.txt   \n",
       "1             cher_pirate.txt   \n",
       "2              cher_stars.txt   \n",
       "3          cher_thesedays.txt   \n",
       "4         cher_lovesohigh.txt   \n",
       "\n",
       "                                          clean_text  sentiment_score  \n",
       "0  \"come and stay with me\" i'll send away all my ...              3.0  \n",
       "1  \"pirate\" he'll sail on with the summer wind th...             11.0  \n",
       "2  \"stars\" i was never one for saying what i real...             -1.0  \n",
       "3  \"these days\" well i've been out walking and i ...              1.0  \n",
       "4  \"love so high\" every morning i would wake up a...             10.0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_df['clean_text'] = lyrics_df['text'].apply(lambda x: ' '.join(x.lower().replace('\\n', ' ').split()[:20]))\n",
    "lyrics_df[['filename', 'clean_text', 'sentiment_score']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "05d0637d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics_df['artist'] = lyrics_df['filename'].str.extract(r'^([a-zA-Z]+)_')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8646d61",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d50dc49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artist\n",
      "cher    3.528481\n",
      "Name: sentiment_score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "artist_avg = lyrics_df.groupby('artist')['sentiment_score'].mean().sort_values(ascending=False)\n",
    "print(artist_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9567d832",
   "metadata": {},
   "source": [
    "# Question 2/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "175e05b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Top & Bottom Songs for Cher ---\n",
      "\n",
      "Filename: cher_perfection.txt\n",
      "Sentiment Score: 48.0\n",
      "Lyrics (excerpt):\n",
      "\"Perfection\"\n",
      "\n",
      "\n",
      "\n",
      "Hush little Baby, gotta be strong\n",
      "'Cause in this world we are born to fight\n",
      "Be the best, prove them wrong\n",
      "A winner's work is never done, reach the top, number one\n",
      "\n",
      "Oh, perfection\n",
      "You drive me crazy with perfection\n",
      "I've worn my pride as my protection\n",
      "Perfection, ohh\n",
      "\n",
      "I was taught to be tough\n",
      "That the best that you can be ain't enough\n",
      "Crack the whip, sacrifice\n",
      "But I found out paradise had a price\n",
      "\n",
      "I didn't know it then, but oh I know it now\n",
      "You gotta work as hard as love to make th...\n",
      "\n",
      "\n",
      "Filename: cher_mylove.txt\n",
      "Sentiment Score: 45.0\n",
      "Lyrics (excerpt):\n",
      "\"My Love\"\n",
      "\n",
      "\n",
      "\n",
      "When I go away\n",
      "I know my heart can stay with my love\n",
      "It's understood\n",
      "Everywhere with my love\n",
      "My love does it good, whoa\n",
      "My love, oh only my love\n",
      "My love does it good\n",
      "\n",
      "And when the cupboard's bare\n",
      "I'll still find something there with my love\n",
      "It's understood\n",
      "Everywhere with my love\n",
      "My love does it so good, whoa\n",
      "My love, oh only my love\n",
      "My love does it good\n",
      "\n",
      "Only my love, oh only my love\n",
      "Only my love, hold the other things to me\n",
      "Oh, only my love, oh only my love\n",
      "Only my love does it go...\n",
      "\n",
      "\n",
      "Filename: cher_loveandunderstanding.txt\n",
      "Sentiment Score: 44.0\n",
      "Lyrics (excerpt):\n",
      "\"Love And Understanding\"\n",
      "\n",
      "\n",
      "\n",
      "Here, here in this world\n",
      "Where do we go? Where can we turn?\n",
      "When we need some love\n",
      "It seems that love just can't be found\n",
      "Where, where do we stand?\n",
      "When love's supply don't meet love's demand\n",
      "\n",
      "We got enough stars to light the sky at night\n",
      "Enough sun to make to make the whole world bright\n",
      "We got more than enough\n",
      "But there's one thing there's just not enough of\n",
      "\n",
      "Not enough love and understanding\n",
      "We could use some love to ease these troubled times\n",
      "Not enough love and und...\n",
      "\n",
      "\n",
      "Filename: cher_iwalkonguildedsplinters.txt\n",
      "Sentiment Score: -25.0\n",
      "Lyrics (excerpt):\n",
      "\"I Walk On Guilded Splinters\"\n",
      "\n",
      "\n",
      "\n",
      "Some people think they jive me, but I know they must be crazy\n",
      "They can't see their misfortune, or else they're just too lazy\n",
      "Je suie le grand zombie\n",
      "With my yellow chaffen of choisen\n",
      "Ain't afraid of no tomcat and gonna fill my guts with poison\n",
      "I walk through the fire\n",
      "And I'll fly through the smoke\n",
      "I wanna see my enemies\n",
      "On the end of my rope\n",
      "Walk on pins and needles\n",
      "And I see what they can do\n",
      "Walk on guilded splinters\n",
      "With the King of the Zulu\n",
      "\n",
      "Come to me, get it...\n",
      "\n",
      "\n",
      "Filename: cher_outrageous.txt\n",
      "Sentiment Score: -24.0\n",
      "Lyrics (excerpt):\n",
      "\"Outrageous\"\n",
      "\n",
      "\n",
      "\n",
      "Outrageous, outrageous\n",
      "(They say) I'm outrageous\n",
      "It's the rage\n",
      "\n",
      "I'm gonna wear what I will and spend some\n",
      "And I will be dress to kill don'tcha know\n",
      "And when the lights come up\n",
      "I'm ready I'm ready\n",
      "To put on a show with class\n",
      "And if I clash it's cause I want to\n",
      "What a show and I want everyone to know\n",
      "They're gonna fly up, get an eyeful\n",
      "Everything that's craved from me\n",
      "I'm gonna be, I'm gonna be outrageous\n",
      "\n",
      "Outrageous\n",
      "(They say) I'm outrageous\n",
      "It's the rage it's the rage\n",
      "\n",
      "With my lo...\n",
      "\n",
      "\n",
      "Filename: cher_whenyouwalkaway.txt\n",
      "Sentiment Score: -17.0\n",
      "Lyrics (excerpt):\n",
      "\"When You Walk Away\"\n",
      "\n",
      "\n",
      "\n",
      "So leave if you're leaving \n",
      "Go if you must go\n",
      "You won't see me down on my knees\n",
      "Begging you to come back home\n",
      "\n",
      "'Cause I refuse to give you the right\n",
      "To cause these eyes to cry at night\n",
      "I'm well prepared to live my life without you\n",
      "\n",
      "When you walk away\n",
      "You won't walk away with my heart\n",
      "There will be no tears in the dark\n",
      "No crying, no dying, no waiting for you to come back\n",
      "When you're out the door\n",
      "That don't mean I won't breathe no more\n",
      "And I will not beg you to stay\n",
      "When yo...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for artist in lyrics_df['artist'].unique():\n",
    "    print(f\"\\n--- Top & Bottom Songs for {artist.title()} ---\")\n",
    "\n",
    "    artist_df = lyrics_df[lyrics_df['artist'] == artist]\n",
    "\n",
    "    top_3 = artist_df.nlargest(3, 'sentiment_score')\n",
    "    bottom_3 = artist_df.nsmallest(3, 'sentiment_score')\n",
    "\n",
    "    for _, row in pd.concat([top_3, bottom_3]).iterrows():\n",
    "        print(f\"\\nFilename: {row['filename']}\")\n",
    "        print(f\"Sentiment Score: {row['sentiment_score']}\")\n",
    "        print(f\"Lyrics (excerpt):\\n{row['text'][:500]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecea878",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ce0fc3b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [52]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m artist \u001b[38;5;129;01min\u001b[39;00m lyrics_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124martist\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique():\n\u001b[1;32m      3\u001b[0m     subset \u001b[38;5;241m=\u001b[39m lyrics_df[lyrics_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124martist\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m artist]\n\u001b[0;32m----> 4\u001b[0m     \u001b[43msns\u001b[49m\u001b[38;5;241m.\u001b[39mkdeplot(subset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment_score\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39martist\u001b[38;5;241m.\u001b[39mtitle(), fill\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSentiment Score Distributions by Artist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSentiment Score\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sns' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "for artist in lyrics_df['artist'].unique():\n",
    "    subset = lyrics_df[lyrics_df['artist'] == artist]\n",
    "    sns.kdeplot(subset['sentiment_score'], label=artist.title(), fill=True)\n",
    "\n",
    "plt.title(\"Sentiment Score Distributions by Artist\")\n",
    "plt.xlabel(\"Sentiment Score\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8334f4",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "Q: Overall, which artist has the higher average sentiment per song? \n",
    "\n",
    "A: <!-- Your answer here -->\n",
    "\n",
    "---\n",
    "\n",
    "Q: For your first artist, what are the three songs that have the highest and lowest sentiments? Print the lyrics of those songs to the screen. What do you think is driving the sentiment score? \n",
    "\n",
    "A: <!-- Your answer here -->\n",
    "\n",
    "---\n",
    "\n",
    "Q: For your second artist, what are the three songs that have the highest and lowest sentiments? Print the lyrics of those songs to the screen. What do you think is driving the sentiment score? \n",
    "\n",
    "A: <!-- Your answer here -->\n",
    "\n",
    "---\n",
    "\n",
    "Q: Plot the distributions of the sentiment scores for both artists. You can use `seaborn` to plot densities or plot histograms in matplotlib.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fe644d",
   "metadata": {},
   "source": [
    "## Sentiment Analysis on Twitter Descriptions\n",
    "\n",
    "In this section, define two sets of emojis you designate as positive and negative. Make sure to have at least 10 emojis per set. You can learn about the most popular emojis on Twitter at [the emojitracker](https://emojitracker.com/). \n",
    "\n",
    "Associate your positive emojis with a score of +1, negative with -1. Score the average sentiment of your two artists based on the Twitter descriptions of their followers. The average sentiment can just be the total score divided by number of followers. You do not need to calculate sentiment on non-emoji content for this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1a5c1d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cher_followers_data.txt</td>\n",
       "      <td>screen_name name id location followers_count f...</td>\n",
       "      <td>759118.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>robynkonichiwa_followers_data.txt</td>\n",
       "      <td>screen_name name id location followers_count f...</td>\n",
       "      <td>44810.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cher_followers.txt</td>\n",
       "      <td>id 35152213 742153090850164742 149646300645197...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>robynkonichiwa_followers.txt</td>\n",
       "      <td>id 1424055675030806529 1502717352575651840 150...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            filename  \\\n",
       "0            cher_followers_data.txt   \n",
       "1  robynkonichiwa_followers_data.txt   \n",
       "2                 cher_followers.txt   \n",
       "3       robynkonichiwa_followers.txt   \n",
       "\n",
       "                                          clean_text  sentiment_score  \n",
       "0  screen_name name id location followers_count f...         759118.0  \n",
       "1  screen_name name id location followers_count f...          44810.0  \n",
       "2  id 35152213 742153090850164742 149646300645197...              0.0  \n",
       "3  id 1424055675030806529 1502717352575651840 150...              0.0  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "twitter_df['sentiment_score'] = twitter_df['text'].apply(lambda x: tokenize_and_score(x, sentiment_df))\n",
    "twitter_df['clean_text'] = twitter_df['text'].apply(lambda x: ' '.join(x.lower().replace('\\n', ' ').split()[:20]))\n",
    "twitter_df[['filename', 'clean_text', 'sentiment_score']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fc7cae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_df['artist'] = twitter_df['filename'].str.extract(r'^([a-zA-Z]+)_')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33b42bf",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fa884a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artist\n",
      "cher              379559.0\n",
      "robynkonichiwa     22405.0\n",
      "Name: sentiment_score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "artist_avg = twitter_df.groupby('artist')['sentiment_score'].mean()\n",
    "print(artist_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ed1a31",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ef05c850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most popular positive emojis per artist:\n",
      "                emojis  count\n",
      "artist                      \n",
      "cher                ‚ú®  45846\n",
      "robynkonichiwa      ‚ú®   3217\n",
      "\n",
      "Most popular negative emojis per artist:\n",
      "                emojis  count\n",
      "artist                      \n",
      "cher                üíî   2001\n",
      "robynkonichiwa      üíî     72\n"
     ]
    }
   ],
   "source": [
    "def extract_emojis(text):\n",
    "    return [char for char in text if char in emoji.EMOJI_DATA]\n",
    "\n",
    "twitter_df['emojis'] = twitter_df['text'].apply(lambda x: extract_emojis(x))\n",
    "\n",
    "emoji_counts = twitter_df.explode('emojis').groupby(['artist', 'emojis']).size().reset_index(name='count')\n",
    "\n",
    "positive_popular = (\n",
    "    emoji_counts[emoji_counts['emojis'].isin(positive_emojis)]\n",
    "    .sort_values(['artist', 'count'], ascending=[True, False])\n",
    "    .groupby('artist')\n",
    "    .first()\n",
    ")\n",
    "\n",
    "negative_popular = (\n",
    "    emoji_counts[emoji_counts['emojis'].isin(negative_emojis)]\n",
    "    .sort_values(['artist', 'count'], ascending=[True, False])\n",
    "    .groupby('artist')\n",
    "    .first()\n",
    ")\n",
    "\n",
    "print(\"Most popular positive emojis per artist:\\n\", positive_popular)\n",
    "print(\"\\nMost popular negative emojis per artist:\\n\", negative_popular)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb92eb93",
   "metadata": {},
   "source": [
    "Q: What is the average sentiment of your two artists? \n",
    "\n",
    "A: <!-- Your answer here --> \n",
    "\n",
    "---\n",
    "\n",
    "Q: Which positive emoji is the most popular for each artist? Which negative emoji? \n",
    "\n",
    "A: <!-- Your answer here --> \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
